{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Water-mass Methods Package\n",
    "##### BSP Tutorial by Taimoor Sohail and Neill Mackay\n",
    "\n",
    "This tutorial script runs the Binary Space Partitioning Code on NEMO model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the BSP component of the WM_Methods package\n",
    "from WM_Methods import BSP\n",
    "## Other required packages for calculations and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "import xarray as xr\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (deptht: 75, time: 39, x: 362, y: 332)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1980-06-16T20:00:00 ... 2018-06-17T02:00:00\n",
      "  * deptht     (deptht) float64 0.5058 1.556 2.668 ... 5.698e+03 5.902e+03\n",
      "Dimensions without coordinates: x, y\n",
      "Data variables:\n",
      "    nav_lat    (time, y, x) float64 dask.array<chunksize=(1, 332, 362), meta=np.ndarray>\n",
      "    nav_lon    (time, y, x) float64 dask.array<chunksize=(1, 332, 362), meta=np.ndarray>\n",
      "    cell_area  (time, y, x) float64 dask.array<chunksize=(1, 332, 362), meta=np.ndarray>\n",
      "    DiC        (time, deptht, y, x) float64 dask.array<chunksize=(1, 75, 332, 362), meta=np.ndarray>\n",
      "    Cstar      (time, deptht, y, x) float64 dask.array<chunksize=(1, 75, 332, 362), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "chunks = {'time': 1}\n",
    "\n",
    "## Load a text file that includes the list of strings pointing to the relevant data. \n",
    "## This ensures that pull requests don't continuously overwrite hardcoded file paths.\n",
    "\n",
    "filename = 'folders_list.txt'\n",
    "with open(filename) as f:\n",
    "    mylist = f.read().splitlines() \n",
    "\n",
    "## Load the data using xarray. \n",
    "\n",
    "NEMO_historical_tracers = xr.open_mfdataset(mylist[0], decode_times=True, chunks = chunks)\n",
    "NEMO_historical_fluxes = xr.open_mfdataset(mylist[1], decode_times=True, chunks = chunks)\n",
    "NEMO_historical_TS = xr.open_mfdataset(mylist[2], decode_times=True, chunks = chunks)\n",
    "\n",
    "print(NEMO_historical_tracers)\n",
    "\n",
    "## Align time axis across all datasets\n",
    "NEMO_historical_tracers.coords['time'] = NEMO_historical_TS['time']\n",
    "\n",
    "## Grid co-ordinates have an unnecessary 'time' dimension which is dropped here. \n",
    "\n",
    "NEMO_historical_tracers['nav_lat'] = NEMO_historical_tracers['nav_lat'].isel(time=0).drop('time')\n",
    "NEMO_historical_tracers['nav_lon'] = NEMO_historical_tracers['nav_lon'].isel(time=0).drop('time')\n",
    "NEMO_historical_tracers['cell_area'] = NEMO_historical_tracers['cell_area'].isel(time=0).drop('time')\n",
    "NEMO_historical_TS['nav_lat'] = NEMO_historical_TS['nav_lat'].isel(time=0).drop('time')\n",
    "NEMO_historical_TS['nav_lon'] = NEMO_historical_TS['nav_lon'].isel(time=0).drop('time')\n",
    "NEMO_historical_TS['cell_area'] = NEMO_historical_TS['cell_area'].isel(time=0).drop('time')\n",
    "NEMO_historical_fluxes['nav_lat'] = NEMO_historical_fluxes['nav_lat'].isel(time=0).drop('time')\n",
    "NEMO_historical_fluxes['nav_lon'] = NEMO_historical_fluxes['nav_lon'].isel(time=0).drop('time')\n",
    "NEMO_historical_fluxes['cell_area'] = NEMO_historical_fluxes['cell_area'].isel(time=0).drop('time')\n",
    "\n",
    "## Load the 9 basins masks\n",
    "\n",
    "NEMO_mask = xr.open_mfdataset(mylist[3] ,decode_times=True, chunks = chunks)\n",
    "\n",
    "## The mask file has different names for the dimensions and \\\n",
    "# co-ordinates - here I rename the co-ordinates to be the same as the native NEMO names\n",
    "\n",
    "NEMO_mask_renamed = NEMO_mask.rename_dims(dict(lev='deptht'))\\\n",
    ".rename(dict(lev='deptht'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Specify tracers\n",
    "NEMO_C_star = NEMO_historical_tracers.Cstar\n",
    "\n",
    "## Define grid area, volume and depth, as well as land mask\n",
    "NEMO_land_mask = (NEMO_C_star/NEMO_C_star)\n",
    "\n",
    "NEMO_area = NEMO_historical_tracers.cell_area*NEMO_land_mask.isel(deptht=0)\n",
    "NEMO_depth = NEMO_historical_tracers.deptht.values\n",
    "\n",
    "dArea_3D = NEMO_area.expand_dims({'deptht':NEMO_depth.size},axis=1).assign_coords(deptht=NEMO_depth)\n",
    "dSArea_3D = dArea_3D.copy(deep=True)\n",
    "\n",
    "## Define grid area, volume and depth, as well as land mask\n",
    "\n",
    "NEMO_volume = dArea_3D*NEMO_historical_TS.e3t\n",
    "vol500 = NEMO_volume.copy(deep=True)\n",
    "depth_ind = np.argmin(NEMO_depth<500)\n",
    "\n",
    "# Creating 4D mask\n",
    "NEMO_mask_4D = NEMO_mask_renamed.expand_dims({'time':NEMO_historical_tracers.time.size},axis=1)\\\n",
    ".assign_coords(time=NEMO_historical_tracers.time)\n",
    "# Defining 9 basins\n",
    "Basins = NEMO_mask.Basins.values\n",
    "\n",
    "## Specify T,S\n",
    "NEMO_T = NEMO_historical_TS.votemper\n",
    "NEMO_S = NEMO_historical_TS.vosaline\n",
    "\n",
    "## Flatten variables of interest into 2D arrays (time x flattened spatial dimensions)\n",
    "\n",
    "vol_500_flattened = vol500.stack(z=(\"y\", \"x\", \"deptht\"))\n",
    "bigthetao_flattened = (NEMO_T.stack(z=(\"y\", \"x\", \"deptht\")))\n",
    "so_flattened = (NEMO_S.stack(z=(\"y\", \"x\", \"deptht\")))\n",
    "C_star_flattened = (NEMO_C_star.stack(z=(\"y\", \"x\", \"deptht\")))\n",
    "mask_flattened = (NEMO_mask_4D.mask_NEMO.stack(z=(\"y\", \"x\", \"deptht\")))\n",
    "\n",
    "## Shorten their names (not necessary but makes for easier code readability)\n",
    "\n",
    "BA = mask_flattened\n",
    "V500 = vol_500_flattened\n",
    "S = ((so_flattened))\n",
    "T = ((bigthetao_flattened))\n",
    "C_star = C_star_flattened\n",
    "\n",
    "## We set our volume array that will be used for binning to be zero below a given depth\n",
    "\n",
    "vol500[:,depth_ind:,:,:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f54db0f7e004fa782c289ddb6aaa2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d2aac9244b4f998ab526edd8cbdcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nm455\\AppData\\Local\\Temp\\ipykernel_21400\\3464037018.py:69: RuntimeWarning: divide by zero encountered in log10\n",
      "  BSP.draw(x,y,np.log10(v),vals['bounding_box'],'grey', depth=tree_depth)\n"
     ]
    }
   ],
   "source": [
    "## Define the number of BSP bins to output, where number = 2**tree_depth\n",
    "tree_depth = 7\n",
    "# Note: if 2**depth approaches the sample size, the code will not work as an equal volume constraint will become impossible!\n",
    "\n",
    "## Define the window of time over which BSP bins are calculated - this is important when parallelising this process\n",
    "window = 2\n",
    "ti = 0 #int(sys.argv[1])\n",
    "\n",
    "## Create empty arrays that will be filled by the BSP-ised bins\n",
    "## Array sizes are (Basin, time, BSP depth) other than for the bin corners which are\n",
    "## (Basin, time, BSP depth, 4)\n",
    "\n",
    "partitions_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth, 4))\n",
    "T_mean_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "S_mean_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "C_star_mean_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "V_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "V500_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "A_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "hfds_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "wfo_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "Cflux_sum_hist = np.zeros((NEMO_mask_renamed.Basins.size, window,2**tree_depth))\n",
    "\n",
    "## Run a loop over times and basins\n",
    "\n",
    "'''\n",
    "1) calc: This function calculates the BSP bins for any 2D distribution. We input the x,y, and v parameters, as well as the \n",
    "tree depth, first axis to split orthogonal to, and any diagnostics we want to output. \n",
    "We are able to output summed variables for each bin, and meaned variables for each bin.\n",
    "The weight over which the mean is calculated can also be different to the distribution weight, v.\n",
    "\n",
    "2) split: The `calc` function outputs a large nested list, which needs to be split into the constituent diagnostics of interest. \n",
    "Due to the recursive nature of the `calc` function, this splitting must be accomplished in a second function, `split`.\n",
    "The output of the `split` function is a dictionary with BSP box boundaries, summed variables and meaned variables. \n",
    "\n",
    "3) draw: The `draw` function allows us to visualise the BSP boundaries on top of the original distribution. \n",
    "'''\n",
    "\n",
    "time_array = np.zeros(window)\n",
    "\n",
    "for i in tqdm(range(ti*window, (ti+1)*window)):\n",
    "    time_array[int(i-ti*window)] = i\n",
    "    for j in tqdm(range(Basins.size)):\n",
    "        # Get a single timestep as numpy, not dask\n",
    "        ## The x and y axes\n",
    "        x = S[i,:].values\n",
    "        y = T[i,:].values\n",
    "        ## Any tracers to find the weighted mean of\n",
    "        z = C_star[i,:].values\n",
    "        ## The 2D distribution to calculate bins on\n",
    "        v = V500[0,:].values*BA[j,i,:].values\n",
    "               \n",
    "        \n",
    "        # Clean out NAN values\n",
    "        idx = np.isfinite(x)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "        z = z[idx]\n",
    "        v = v[idx]      \n",
    "\n",
    "        ## Calculate the BSP bins\n",
    "        BSP_out = BSP.calc(x,y, v, depth=tree_depth, axis=1, mean=[x,y,v],sum=[v],weight=v)\n",
    "        # Split the output into constituent diagnostics\n",
    "        vals = BSP.split(BSP_out, depth=tree_depth)\n",
    "        ## Draw the BSP bins onto original grid\n",
    "\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        ax = fig.add_subplot(1,2,1)\n",
    "        BSP.draw(x,y,np.log10(v),vals['bounding_box'],'grey', depth=tree_depth)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        cbar.set_label('Distribution weight')\n",
    "            \n",
    "        ax = fig.add_subplot(1,2,2)\n",
    "        BSP.draw(vals['meaned_vals'][:,0],vals['meaned_vals'][:,1],vals['summed_vals'],vals['bounding_box'],'red', depth=tree_depth, cmap=plt.cm.viridis)\n",
    "        cbar = plt.colorbar()\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        cbar.set_label('Summed weight')\n",
    "        \n",
    "        if i == 1:\n",
    "            plt.show()\n",
    "            \n",
    "        ## The BSP results come out as a long list of different numbers in the format\n",
    "        ## [bounds, xmean, ymean, zmean, vsum, csum,  asum, usum, wsum, qsum], where bounds \n",
    "        ## itself is a tuple (x0,y0,x1,y1).\n",
    "        ## Below, we split the list into arrays of each variable\n",
    "\n",
    "#        bsp_flat = list(itertools.chain(*BSP_out))\n",
    "#\n",
    "#        while (len(bsp_flat) <= 2**tree_depth):\n",
    "#            bsp_flat = list(itertools.chain(*bsp_flat))\n",
    "#        partitions_hist[j,int(i-ti*window),:,:] = np.array(bsp_flat[::11])\n",
    "#        T_mean_hist[j,int(i-ti*window),:] = np.array(bsp_flat[1::11])\n",
    "#        S_mean_hist[j,int(i-ti*window),:] = np.array(bsp_flat[2::11])\n",
    "#        C_star_mean_hist[j,int(i-ti*window),:] = np.array(bsp_flat[3::11])\n",
    "#        V500_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[4::11])\n",
    "#        V_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[5::11])\n",
    "#        A_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[6::11])\n",
    "#        hfds_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[7::11])\n",
    "#        wfo_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[8::11])\n",
    "#        Cflux_sum_hist[j,int(i-ti*window),:] = np.array(bsp_flat[9::11])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We redefine each outputted numpy array as an xarray DataArray with the goal of saving it as a netCDF file\n",
    "\n",
    "da_partitions_hist = xr.DataArray(data = partitions_hist, dims = [\"Basin\",\"Time\", \"Depth\", \"Coords\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth= np.arange(2**depth), Coords = np.arange(4)),\n",
    "                        attrs=dict(description=\"[x0,y0,xmax,ymax] bounds of BSP framework\", variable_id=\"Partitions\"))\n",
    "da_S_mean_hist = xr.DataArray(data = S_mean_hist, dims = [\"Basin\", \"Time\", \"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Mean Salinity\", units=\"g/kg\", variable_id=\"S\"))\n",
    "da_T_mean_hist = xr.DataArray(data = T_mean_hist, dims = [\"Basin\", \"Time\",\"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Mean Temperature\", units=\"K\", variable_id=\"T\"))\n",
    "da_C_star_mean_hist = xr.DataArray(data = C_star_mean_hist, dims = [\"Basin\", \"Time\",\"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Mean C_star\", units=\"mmol-C/m3\", variable_id=\"C_star\"))\n",
    "da_V_sum_hist = xr.DataArray(data = V_sum_hist, dims = [\"Basin\", \"Time\",\"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Total Volume\", units=\"m^3\", variable_id=\"Basin V_sum\"))\n",
    "da_A_sum_hist = xr.DataArray(data = A_sum_hist, dims = [\"Basin\", \"Time\",\"Depth\"],\n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Total Area\", units=\"m^2\", variable_id=\"Basin A_sum\"))\n",
    "da_hfds_sum_hist = xr.DataArray(data = hfds_sum_hist, dims = [\"Basin\", \"Time\",\"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Heat Flux\", units=\"W\", variable_id=\"hfds\"))\n",
    "da_wfo_sum_hist = xr.DataArray(data = wfo_sum_hist, dims = [\"Basin\", \"Time\",\"Depth\"], \n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"FW Flux\", units=\"kg/s\", variable_id=\"wfo\"))\n",
    "da_Cflux_sum_hist = xr.DataArray(data = Cflux_sum_hist, dims = [\"Basin\", \"Time\",\"Depth\"],\n",
    "                           coords=dict(Basin = Basins, Time = time_array, Depth=np.arange(2**depth)),\n",
    "                        attrs=dict(description=\"Carbon Flux\", units=\"mmol-C/s\", variable_id=\"Cflux\"))\n",
    "\n",
    "## Input all xarray DataArrays into a DataSet\n",
    "\n",
    "ds_BSP = xr.Dataset()\n",
    "ds_BSP['Partitions_hist'] = da_partitions_hist\n",
    "ds_BSP['T_mean_hist'] = da_T_mean_hist\n",
    "ds_BSP['S_mean_hist'] = da_S_mean_hist\n",
    "ds_BSP['C_star_mean_hist'] = da_C_star_mean_hist\n",
    "ds_BSP['V_sum_hist'] = da_V_sum_hist\n",
    "ds_BSP['A_sum_hist'] = da_A_sum_hist\n",
    "ds_BSP['hfds_sum_hist'] = da_hfds_sum_hist\n",
    "ds_BSP['wfo_sum_hist'] = da_wfo_sum_hist\n",
    "ds_BSP['Cflux_sum_hist'] = da_Cflux_sum_hist\n",
    "\n",
    "ds_BSP.to_netcdf(mylist[4]+'BSP_NEMO_TS_hist_area_%i_%i.nc' %(ti*window, (ti+1)*window-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSP_out"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
